from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.functions import col

from pyspark.sql.functions import col, count, countDistinct, when, concat
from pyspark.sql import Window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
from pyspark.sql.functions import udf
from pyspark.sql import Row


spark = SparkSession.builder.appName("mbd-prj").getOrCreate()

wildchat_df = spark.read.parquet("/user/s3485269/WildChat-1M-Full")

# filter out rows where 'country' is null
wildchat_df = wildchat_df.filter(F.col("country").isNotNull())

print(wildchat_df.show())

# open the parquet file
country_state_df = spark.read.parquet("/user/s3485269/country_state_timezone.parquet")
print(country_state_df.count()) # 2433

# join condition to handle cases when 'state' is null
wildchat_df = wildchat_df.join(
    country_state_df,
    (
        (wildchat_df["country"] == country_state_df["ccountry"]) &
        (
            (wildchat_df["state"] == country_state_df["sstate"]) |
            (wildchat_df["state"].isNull() & country_state_df["sstate"].isNull())
        )
    ),
    "left"
)


# Select the required columns and filter to show rows where 'timezone' is null, supposed to be 1262 (only a small fraction)
wildchat_df.select("country", "state", "timezone").filter(F.col("timezone").isNull()).count() # 1262

# drop ccountry and sstate columns
wildchat_df = wildchat_df.drop("ccountry", "sstate")
wildchat_df.columns

# Shift the timestamp by adding the GMT offset hours
wildchat_df = wildchat_df.withColumn(
	"offseted_timestamp",
	(F.unix_timestamp("timestamp") + F.col("gmt_offset.hours") * 3600 + F.col("gmt_offset.minutes") * 60).cast("timestamp")
)

##############

# Plot the percentage of harmful conversations by time of the day
harmful_df = wildchat_df.select("conversation_hash", "conversation", "openai_moderation", "turn", "offseted_timestamp")

harmful_df = harmful_df.withColumn("hour_bin", F.hour("offseted_timestamp"))

# we determine a conversation is harmful if any message in the conversation is flagged as harmful
# create a column to check if the conversation is harmful
harmful_df = harmful_df.withColumn("is_harmful", F.array_contains(F.col("openai_moderation.flagged"), True))

# Count the number of harmful conversations
num_harmful = harmful_df.filter(F.col("is_harmful")).count()
print(f"Number of harmful conversations: {num_harmful:,}")

# Count the number of harmful conversations by hour
harmful_hour_counts = harmful_df.filter(F.col("is_harmful")).groupBy("hour_bin").agg(F.count("*").alias("num_harm")).orderBy("hour_bin")

# Count the total number of conversations by hour
total_hour_counts = harmful_df.groupBy("hour_bin").agg(F.count("*").alias("total_harm")).orderBy("hour_bin")

# Calculate the percentage of harmful conversations by hour
hourly_percentage = harmful_hour_counts.join(total_hour_counts, "hour_bin")
hourly_percentage = hourly_percentage.withColumn("percentage", (F.col("num_harm") / F.col("total_harm")).cast("double") * 100)
hourly_percentage = hourly_percentage.orderBy("hour_bin")

# Plot the number of of harmful conversations by hour
hourly_percentage = hourly_percentage.withColumn("harm_percentage", (F.col("num_harm") / num_harmful).cast("double") * 100)

########
# In order to calculate the percentage of generated harmful messages of a model
# We have to first loop through the conversation and corresponding openai_moderation
# We only count the number of harmful message generated by the assistant

harmful_df = wildchat_df.select("conversation_hash", "conversation", "openai_moderation", "turn", "model", "offseted_timestamp")

harmful_df = harmful_df.withColumn("model_family",
	F.when(F.col("model").like("gpt-3.5%"), "GPT-3.5")
	 .when(F.col("model").like("gpt-4%"), "GPT-4")
	 .otherwise("Other")
)

harmful_df = harmful_df.withColumn("hour_bin", F.hour("offseted_timestamp"))

# Explode the conversation array to get one row per message
exploded_df = harmful_df.select(
	"model_family",
	F.explode(F.arrays_zip("conversation.role", "openai_moderation.flagged", "openai_moderation.categories")).alias("message"),
	"hour_bin"
)

# Filter out only assistant messages that are flagged
# exploded_df = exploded_df.filter((F.col("message.role") == "assistant") & (F.col("message.flagged") == True))
exploded_df = exploded_df.filter((F.col("message.role") == "assistant"))

# Extract harmful flags
def extract_harmful_flags_from_categories(categories):
	if categories:
		harmful_flags = []
		for key, value in categories.asDict().items():
			if value and key not in harmful_flags:
				harmful_flags.append(key)
		return harmful_flags
	else:
		raise ValueError("Categories is empty")
	
extract_harmful_flags_from_categories_udf = F.udf(extract_harmful_flags_from_categories, ArrayType(StringType()))

# Apply the UDF to extract harmful flags dynamically
exploded_df = exploded_df.withColumn(
	"harmful_flags", 
	extract_harmful_flags_from_categories_udf(F.col("message.categories"))
)

# Explode the harmful flags
exploded_df_1 = exploded_df.withColumn("harmful_flag", F.explode("harmful_flags"))

# Count number of harmful messages by model and harmful flag
harmful_flags = exploded_df_1.groupBy("model_family", "harmful_flag").agg(F.count("*").alias("num_harm")).orderBy(F.desc("num_harm"))

# Count total number of harmful messages by model
total_harmful_messages = exploded_df.groupBy("model_family").agg(F.count("*").alias("total_harm")).orderBy(F.desc("total_harm"))

# Calculate the percentage of harmful messages by model
model_percentage = harmful_flags.join(total_harmful_messages, "model_family")
model_percentage = model_percentage.withColumn("percentage", (F.col("num_harm") / F.col("total_harm")).cast("double") * 100)

# Sort by model family and percentage
model_percentage = model_percentage.orderBy("model_family", F.desc("percentage"))
model_percentage.show(30)

